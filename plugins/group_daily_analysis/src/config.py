import json
from pathlib import Path
from typing import Optional, List, Any
from pydantic import BaseModel, Field
from nonebot.log import logger

class LLMConfig(BaseModel):
    # openai_compatible: /chat/completions (OpenAI 兼容接口，保持现状)
    # google_ai_studio: Gemini Developer API (AI Studio 官方接口 generateContent)
    provider: str = "openai_compatible"

    # OpenAI compatible config
    api_key: str
    base_url: str = "https://api.openai.com/v1"

    # Google AI Studio config (optional)
    # 为空时回退使用 api_key
    google_api_key: Optional[str] = None
    google_base_url: str = "https://generativelanguage.googleapis.com/v1beta"

    model: str = "gpt-3.5-turbo"
    timeout: float = 60.0
    proxy: Optional[str] = None

class PluginConfig(BaseModel):
    llm: LLMConfig
    max_concurrent_tasks: int = 3
    max_messages: int = 1000
    analysis_days: int = 1
    auto_analysis_time: str = "09:00"
    enable_auto_analysis: bool = False
    bot_qq_ids: List[str] = []
    enable_user_card: bool = False
    report_template: str = "scrapbook"
    min_messages_threshold: int = 50
    watermark_text: str = "Generated by HakuBot · Refactored by Hakuchumu"
    
    # Analysis toggles
    topic_analysis_enabled: bool = True
    user_title_analysis_enabled: bool = True
    golden_quote_analysis_enabled: bool = True
    
    # Analysis limits
    max_topics: int = 5
    max_user_titles: int = 8
    max_golden_quotes: int = 5
    
    # Map-Reduce Settings
    max_input_length: int = 6000  # 字符数阈值，超过则分块
    
    # Prompts
    topic_analysis_prompt: str
    topic_merge_prompt: str = "以下是分段分析得到的群聊话题列表，请将它们合并、去重，并总结出全天最重要的 **{max_topics}** 个话题。\n\n## 待合并话题：\n{topics_text}\n\n---\n\n## 要求：\n1. 合并相似话题（如上午讨论'游戏'，下午讨论'游戏'，合并为一个）\n2. 保留细节最丰富、讨论最热烈的话题\n3. 返回格式与原话题分析一致，使用 JSON 格式。"
    
    user_title_analysis_prompt: str
    
    golden_quote_analysis_prompt: str
    golden_quote_merge_prompt: str = "以下是分段分析选出的候选金句，请从中通过 PK 决选出最逆天的 **{max_golden_quotes}** 句。\n\n## 候选金句：\n{quotes_text}\n\n---\n\n## 要求：\n1. 严格按照'逆天指数'排序\n2. 去除重复或相似的句子\n3. 返回格式与原金句分析一致，使用 JSON 格式。"

CONFIG_PATH = Path(__file__).parent.parent / "config.json"

def load_config() -> PluginConfig:
    if not CONFIG_PATH.exists():
        raise FileNotFoundError(f"Config file not found: {CONFIG_PATH}")
    
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # Try to load ai_assistant config if API key is placeholder
    if data["llm"]["api_key"] == "YOUR_API_KEY":
        try:
            ai_config_path = Path(__file__).parent.parent.parent / "ai_assistant" / "config.json"
            if ai_config_path.exists():
                with open(ai_config_path, "r", encoding="utf-8") as f:
                    ai_data = json.load(f)
                    data["llm"]["api_key"] = ai_data.get("api_key", "")
                    data["llm"]["base_url"] = ai_data.get("base_url", data["llm"]["base_url"])
                    data["llm"]["model"] = ai_data.get("chat_model", data["llm"]["model"])
                    data["llm"]["proxy"] = ai_data.get("proxy", data["llm"]["proxy"])

                    # Sync provider switch + google fields (optional)
                    if "provider" in ai_data:
                        data["llm"]["provider"] = ai_data.get("provider", data["llm"].get("provider", "openai_compatible"))
                    if "google_api_key" in ai_data:
                        data["llm"]["google_api_key"] = ai_data.get("google_api_key", data["llm"].get("google_api_key"))
                    if "google_base_url" in ai_data:
                        data["llm"]["google_base_url"] = ai_data.get("google_base_url", data["llm"].get("google_base_url"))

                    logger.info("已从 ai_assistant 插件加载 LLM 配置")
            else:
                logger.warning("未找到 ai_assistant 配置文件，请手动配置 LLM API")
        except Exception as e:
            logger.warning(f"加载 ai_assistant 配置失败: {e}，请手动配置 LLM API")

    return PluginConfig(**data)

def save_config(config: PluginConfig):
    with open(CONFIG_PATH, "w", encoding="utf-8") as f:
        json.dump(config.dict(), f, indent=4, ensure_ascii=False)

plugin_config = load_config()
